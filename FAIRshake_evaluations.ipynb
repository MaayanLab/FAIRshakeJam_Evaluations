{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# FAIRshake Evaluation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created by Brian Schilder, 07/22/2018\n\nPython Version Info:\n3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 03:03:55) \n[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "print(\"Created by Brian Schilder, 07/22/2018\\n\")\n",
    "import sys\n",
    "print(\"Python Version Info:\")\n",
    "print(sys.version)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Questions\n",
    "- Average score for each question\n",
    "- Standard deviation for each answer\n",
    "- Correlation between questions\n",
    "- Evals per person, max evals, and distribution\n",
    "- Are some people generally harsher\n",
    "- Which answers tended to have comments\n",
    "- FAIRest tools vs. Least FAIR tools\n",
    "\n",
    "## FAIRshake Questions\n",
    "1. DESCRIPTION : The tool has a unique name and an informative description.\n",
    "2. DOWNLOAD : The tool can be freely downloaded or accessed from a webpage.\n",
    "3. ONTOLOGY : The tool utilizes a community-accepted ontology.\n",
    "4. TUTORIAL : Tutorials for the tool are available on the tool‚Äôs homepage.\n",
    "5. SOURCE CODE : Source code is shared in a public repository and is documented.\n",
    "6. VERSIONING : Previous versions of the tool are made available.\n",
    "7. CONTACT: Contact information is provided for the creator(s) of the tool and information describing how to cite the tool is provided.\n",
    "8. API : The tool can be accessed programmatically through an API and follows community standards for open APIs.\n",
    "9. LICENSE : Licensing information is provided on the tool's homepage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3789, 32)\nIndex(['Unnamed: 0', 'Unnamed: 0.1', 'user_id', 'resource_id', 'q_id',\n       'answer', 'url_comment', 'comment', 'project_id_x', 'test_x', 'num',\n       'version', 'content', 'F', 'A', 'I', 'R', 'res_type', 'test_y',\n       'username', 'first_name', 'middle_initial', 'last_name', 'test_x.1',\n       'affiliation', 'orcid', 'resource_name', 'url', 'resource_type',\n       'description', 'project_id_y', 'test_y.1'],\n      dtype='object')\n+++++++++++++++\n421 evaluations total.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "import pandas as pd\n",
    "fair = pd.read_csv(\"merged_FAIRshake_evals.csv\", index_col=None)\n",
    "fair['q_id'] = pd.Categorical(fair['q_id'])\n",
    "print(fair.shape)\n",
    "print(fair.columns)\n",
    "\n",
    "qDict = {1:'1. Description', 2:'2. Download', 3:'3. Ontology', 4:'4. Tutorial', \n",
    "         5:'5. Source Code', 6:'6. Versioning', 7:'7. Contact', 8:'8. API', 9:'9. License'}\n",
    "fair[\"q_name\"] = fair['q_id'].map(qDict)\n",
    "print(\"+++++++++++++++\")\n",
    "print(str(int(len(fair)/9)) + \" evaluations total.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average score for each question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "sn.set(font_scale=1.5)\n",
    "import matplotlib.pyplot as plt\n",
    "sn.set_style(\"whitegrid\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 6)) #default figsize = (8,6) \n",
    "\n",
    "fair.answer.unique()\n",
    "fair[\"answer01\"] = fair.answer.map(dict(yes=1, no=0, yesbut=.75, nobut=.25))\n",
    "avg_per_q = fair.groupby('q_name')['q_name','answer01'].mean()\n",
    "std_per_q = fair.groupby('q_name')['q_name','answer01'].std()\n",
    "avg_per_q.rename(columns={'answer01':'answer01_mean'}, inplace=True)\n",
    "avg_per_q = avg_per_q.sort_values(by='answer01_mean', ascending=False)\n",
    "\n",
    "# plot\n",
    "# fair = fair.merge(avg_per_q, on='q_id')\n",
    "g = sn.barplot(data=fair, x='q_name', y='answer01', order=avg_per_q.index)\n",
    "# ax = sn.violinplot(data=fair, x='q_id', y='answer01', order=avg_per_q.index)\n",
    "g.set(ylabel='Answer \\n(yes=1, yesbut=.75, nobut=.25, no=0)', xlabel='Question ID', title=\"Average answer for each question\")\n",
    "plt.xticks(rotation=45, ha='center')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency of each of the 4 score options for each question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6)) #default figsize = (8,6)\n",
    "\n",
    "scores4 = fair.groupby(['q_name','answer'])['answer'].count()\n",
    "scores4.name = 'answerCount'\n",
    "scores4 = scores4.reset_index()\n",
    "# Calculate relative frequency \n",
    "totalCounts = scores4.groupby(by='q_name')['answerCount'].sum().reset_index()\n",
    "totalCounts = totalCounts.rename(columns={'answerCount':'totalCounts_per_q_name'})\n",
    "scores4 = scores4.merge(totalCounts, on='q_name')\n",
    "scores4['answerCount_relFreq'] = scores4['answerCount']/scores4['totalCounts_per_q_name']\n",
    "# Plot\n",
    "palette ={\"no\":\"red\",\"nobut\":\"lightcoral\", \"yes\":\"green\", \"yesbut\":\"limegreen\"}\n",
    "g = sn.barplot(data=scores4, x='q_name', y='answerCount_relFreq', hue='answer', palette=palette)\n",
    "g.set(ylabel='Relative Frequency', xlabel='Question ID', title=\"Frequency of each answer, by question\")\n",
    "g.legend()\n",
    "plt.xticks(rotation=45, ha='center')\n",
    "plt.ylim((0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations between questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is some correlation between q1 (Description) and q2 (Download).\n"
     ]
    }
   ],
   "source": [
    "# fig, ax = plt.subplots(figsize=(8, 6)) #default figsize = (8,6)\n",
    "\n",
    "corrDict={}\n",
    "for q in fair['q_name']:\n",
    "    sub = fair.loc[fair['q_name']==q]['answer01']\n",
    "    corrDict[q] = sub.values\n",
    "corrMatrix = pd.DataFrame(corrDict).corr()\n",
    "\n",
    "# g = sn.heatmap(corrMatrix, annot=True, cmap='seismic')\n",
    "g = sn.clustermap(data=corrMatrix, cmap='seismic').fig.suptitle('\\nCorrelations Between Question Scores (Clustered)', size=14) \n",
    "# g.set(ylabel='Question ID', xlabel='Question ID', title=\"Correlations between all questions\")\n",
    "plt.show()\n",
    "\n",
    "print(\"There is some correlation between q1 (Description) and q2 (Download).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KDE Density Plots of Evaluations Per Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6)) #default figsize = (8,6)\n",
    "\n",
    "evalCounts = [len(x) for x in fair.groupby('user_id')['resource_name'].unique()]\n",
    "avgCounts = sum(evalCounts)/len(evalCounts)\n",
    "\n",
    "g = sn.distplot(evalCounts, bins=100, norm_hist=False)\n",
    "g.set(xlabel='Number of Evaluations', ylabel='Density', \n",
    "       title=\"KDE Density : Evaluations/person\\n Mean = \"+str(round(avgCounts,2))+\"  \\n Max = \"+str(max(evalCounts)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are some raters harsher than others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluators gave a average FAIR score of 0.58 (on a 0-1 scale).\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6)) #default figsize = (8,6)\n",
    "\n",
    "avgRating = fair.groupby('user_id')['user_id','answer01'].mean() \n",
    "\n",
    "meanMean = avgRating['answer01'].mean()\n",
    "g = sn.distplot(avgRating['answer01'], bins=50, color='green', norm_hist=False)\n",
    "g.set(xlabel='Mean Evaluation Score\\n (yes=1, yesbut=.75, nobut=.25, no=0)', ylabel='Density', \n",
    "       title=\"KDE Density : Average Evaluation Score/Evaluator\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Evaluators gave a average FAIR score of \"+str(round(meanMean,2)) + \" (on a 0-1 scale).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which answers tended to have the most comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions 1. Description and 7. Contact most frequently had comments.\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6)) #default figsize = (8,6)\n",
    "\n",
    "commentDict={}\n",
    "for q in fair['q_name'].unique():\n",
    "    sub = fair.loc[fair['q_name']==q]['comment']\n",
    "    notEmpty = sum(~sub.isnull())/len(sub.isnull())\n",
    "    commentDict[q] = notEmpty\n",
    "commentDF = pd.Series(commentDict).sort_values(ascending=False)\n",
    "\n",
    "g = sn.barplot(x=commentDF.index, y=commentDF )\n",
    "g.set(xlabel='Question ID', ylabel='Relative Frequency', \n",
    "       title=\"Frequency of Comments for Each Question\")\n",
    "plt.xticks(rotation=45, ha='center')\n",
    "plt.show()\n",
    "\n",
    "print(\"Questions \"+commentDF.index[0]+\" and \"+commentDF.index[1]+ \" most frequently had comments.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The FAIRest of the them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAIRest resources (score = 1.0)\n                   resource_name  answer01\n0  Saccharomyces Genome Database       1.0\n1                         Canvas       1.0\n2                    HaploGrep 2       1.0\n\n\nLeast FAIR resources ( score = 0.0).\n           resource_name  answer01\n385           NetExplore       0.0\n386                Ka-me       0.0\n387               SpeeDB       0.0\n388               Evoker       0.0\n389               Mcheza       0.0\n390      ConsensusPathDB       0.0\n391                 PAIR       0.0\n392                 gCUP       0.0\n393  Bridging the scales       0.0\n394           RepMaestro       0.0\n395              MeMotif       0.0\n396                  SVA       0.0\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6)) #default figsize = (8,6)\n",
    "\n",
    "resourceScores = fair.groupby('resource_name')['answer01'].mean().sort_values(ascending=False).reset_index()\n",
    "FAIRest = resourceScores.loc[resourceScores['answer01']==1,:]\n",
    "leastFAIR  = resourceScores.loc[resourceScores['answer01']==0,:]\n",
    "\n",
    "g = sn.barplot(data=resourceScores, x='resource_name', y='answer01')\n",
    "g.set(xlabel='Resource (n = '+str(len(resourceScores))+')', ylabel='FAIR score\\n(yes=1, yesbut=.75, nobut=.25, no=0)', \n",
    "       title=\"Mean FAIRness Scores for each resource\")\n",
    "g.set(xticklabels=[])\n",
    "plt.show()\n",
    "\n",
    "print(\"FAIRest resources (score = 1.0)\")\n",
    "print(FAIRest)\n",
    "print(\"\\n\")\n",
    "print(\"Least FAIR resources ( score = 0.0).\")\n",
    "print(leastFAIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time per Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of FAIR scores across resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sn.distplot(resourceScores['answer01'], bins=50, color='green', norm_hist=False)\n",
    "g.set(xlabel='Mean Evaluation Score\\n (yes=1, yesbut=.75, nobut=.25, no=0)', ylabel='Density', \n",
    "       title=\"KDE Density : FAIR Score per Resource\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earliest timeStamp = 19/Jul/2018:14:18:54 +0000\nLatest timeStamp = 23/Jul/2018:20:07:44 +0000\nProcessing eval times\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "# Import and format Time Stamp data (from Lily Wang)\n",
    "timeData=[]\n",
    "with open('fairshakejam-timestamp.txt') as f:\n",
    "    flines = f.readlines() \n",
    "    import ast\n",
    "    for line in flines:\n",
    "        if line.startswith('{'):\n",
    "            timeData.append(ast.literal_eval(line.strip('\\n')))\n",
    "len(timeData)\n",
    "timeDF = pd.DataFrame(timeData) \n",
    "timeDF.head()\n",
    "len(timeDF['ip'].unique()) \n",
    "print(\"Earliest timeStamp = \"+timeDF['timestamp'][0])\n",
    "print(\"Latest timeStamp = \"+timeDF['timestamp'].iloc[-2])\n",
    "\n",
    "\n",
    "# Conditionally add new col values\n",
    "conditions = [ \n",
    "    (timeDF['req'].str.split(' ').str[-1].str.startswith('/newevaluation')),\n",
    "    (timeDF['req'].str.split(' ').str[-1].str.startswith('/evaluationsubmitted'))\n",
    "]\n",
    "choices = ['evalStart','evalEnd']\n",
    "timeDF['marker'] = np.select(conditions, choices, default='')\n",
    "# Parse resource ID\n",
    "timeDF['resourceID'] = timeDF[timeDF['req'].str.contains('resourceid=')]['req'].str.split('resourceid=').str[-1]\n",
    "\n",
    "print('Processing eval times')\n",
    "evalLengths={}\n",
    "# Loop through each person\n",
    "for ip in timeDF['ip'].unique():\n",
    "    # print(\"Processing ip - \"+ip)\n",
    "    sub = timeDF.loc[timeDF['ip']==ip]\n",
    "    sub['resourceID'] = sub['resourceID'].fillna(method='ffill')\n",
    "    for resource in sub['resourceID'].unique():\n",
    "        rsub = sub.loc[sub['resourceID']==resource]\n",
    "        evalStart = rsub[rsub['marker']=='evalStart']['timestamp'].values\n",
    "        evalEnd = rsub[rsub['marker'] =='evalEnd']['timestamp'].values  \n",
    "        # evalStart = ['20/Jul/2018:14:13:33 +0000'] \n",
    "        # evalEnd = ['20/Jul/2018:14:18:46 +0000']\n",
    "        if len(evalStart)!=0 and len(evalEnd)!=0:\n",
    "            # only get timestamps that start with day\n",
    "            evalStart = [item for item in evalStart if item[0].isdigit()]\n",
    "            evalEnd = [item for item in evalEnd if item[0].isdigit()]\n",
    "            # Convert to time_struct\n",
    "            timeStart = time.strptime(evalStart[0], '%d/%b/%Y:%H:%M:%S +%f')\n",
    "            timeEnd = time.strptime(evalEnd[0], '%d/%b/%Y:%H:%M:%S +%f') \n",
    "            timeLength = (time.mktime(timeEnd) - time.mktime(timeStart)) / 60 # Return difference in minutes\n",
    "            # Symbol translation at:  http://strftime.org/\n",
    "            evalLengths[resource] = timeLength\n",
    "len(evalLengths)\n",
    "# Save file to csv\n",
    "timeDF.to_csv('fairshakejam-timestamp_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean evaluation time = 28.48 minutes.\nMode evaluation time = 1.05 minutes.\nMedian evaluation time = 3.55 minutes.\nMin evaluation time = 8.0 seconds.\nMax evaluation time = 21.71 hours.\n"
     ]
    }
   ],
   "source": [
    "# Report summary stats\n",
    "avgEvalTime = sum(evalLengths.values()) / len(evalLengths.values())\n",
    "evalLengthsDF = pd.Series(evalLengths, name='evalLength').reset_index().rename(columns={'index':'resourceID'})\n",
    "lst = list(evalLengths.values())\n",
    "mode  = max(set(lst), key=lst.count)\n",
    "lst_sort = sorted(lst)\n",
    "median = lst_sort[int((len(lst_sort)/2)) ]\n",
    "\n",
    "print(\"Mean evaluation time = \"+str(round(avgEvalTime,2))+\" minutes.\")\n",
    "print(\"Mode evaluation time = \"+str(mode)+\" minutes.\")\n",
    "print(\"Median evaluation time = \"+str(median)+\" minutes.\") \n",
    "print(\"Min evaluation time = \"+str(round(min(evalLengths.values())*60,2))+\" seconds.\")            \n",
    "print(\"Max evaluation time = \"+str(round( max(evalLengths.values())/60,2))+\" hours.\") \n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6)) #default figsize = (8,6) \n",
    "g = sn.distplot(evalLengthsDF['evalLength'] )\n",
    "g.set(xlabel='Evaluation Length (minutes)', ylabel='Density', \n",
    "       title=\"KDE Density :\\nDistribution of time per evaluation\")\n",
    "\n",
    "fig2, ax2 = plt.subplots(figsize=(8, 6)) #default figsize = (8,6) \n",
    "# Log values to see distribution better\n",
    "g2 = sn.distplot( np.log( evalLengthsDF['evalLength']),  kde_kws={'clip': (0.0,  max(np.log( evalLengthsDF['evalLength'])))})\n",
    "g2.set(xlabel='log( Evaluation Length (minutes) )', ylabel='Density', \n",
    "       title=\"KDE Density :\\nDistribution of Time per Evaluation (Logged)\")\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
